{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining\n",
    "\n",
    "## Text Mining\n",
    "\n",
    "1. A simple text cleaning definition.\n",
    "2. Feature Extraction.\n",
    "3. Training the models.\n",
    "4. Testing the models.\n",
    "\n",
    "For training K-Means models, the following 30 sentences were collected from 3 categories, namely Cricket, Artificial Intelligence and Chemistry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cricket is a bat and ball game played between two teams of eleven players each on a cricket field.\n",
    "2. Each phase of play is called an innings during which one team bats, attempting to score as many runs as possible.\n",
    "3. The teams have one or two innings apiece and, when the first innings ends, the teams swap roles for the next innings\n",
    "4. Before a match begins, the two team captains meet on the pitch for the toss of a coin to determine which team will bat first.\n",
    "5. Two batsmen and eleven fielders then enter the field and play begins when a member of the fielding team, known as the bowler, delivers the ball.\n",
    "6. The most common dismissal in cricket match are bowled, when the bowler hits the stumps directly with the ball and dislodges the bails. Batsman gets out.\n",
    "7. Runs are scored by two main methods: either by hitting the ball hard enough for it to cross the boundary, or by the two batsmen swapping ends.\n",
    "8. The main objective of each team is to score more runs than their opponents.\n",
    "9. If the team batting last is all out having scored fewer runs than their opponents, they are said to have \"lost by n runs\".\n",
    "10. The role of striker batsman is to prevent the ball from hitting the stumps by using his bat and, simultaneously, to strike it well enough to score runs\n",
    "11. Artificial intelligence is intelligence exhibited by machines, rather than humans or other animals. \n",
    "12. the field of AI research defines itself as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of success at some goal\n",
    "13. The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner.\n",
    "14. Natural language processing[77] gives machines the ability to read and understand human language and extract intelligence from it.\n",
    "15. AI researchers developed sophisticated mathematical tools to solve specific subproblems. These tools are truly scientific, in the sense that their results are both measurable and verifiable.\n",
    "16. An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success.\n",
    "17. AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science.\n",
    "18. Recent advancements in AI, and specifically in machine learning, have contributed to the growth of Autonomous Things such as drones and self-driving cars.\n",
    "19. AI research was revived by the commercial success of expert systems,[28] a form of AI program that simulated the knowledge and analytical skills of human experts.\n",
    "20. Advanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers enabled advances in machine learning and perception.\n",
    "21. A compound is a pure chemical substance composed of more than one element and the properties of a compound bear little similarity to those of its elements.\n",
    "22. Since the properties of an element are mostly determined by its electron configuration, the properties of the elements likewise show recurring patterns or periodic behaviour.\n",
    "23. The property of inertness of noble gases makes them very suitable in chemicals where reactions are not wanted.\n",
    "24. The atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential and preferred oxidation state.\n",
    "25. The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus\n",
    "26. The atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space called the electron cloud.\n",
    "27. A chemical reaction is a transformation of some substances into one or more different substances.\n",
    "28. Chemistry is sometimes called the central science because it bridges other natural sciences, including physics, geology and biology.\n",
    "29. Chemistry includes topics such as the properties of individual atoms and how atoms form chemical bonds to create chemical compounds.\n",
    "30. Chemistry is a branch of physical science that studies the composition, structure of atoms, properties and change of matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Work Fellow__:\n",
    "\n",
    "\n",
    "Texts  ==>  Stop words removal ==> Punctuation free ==> Word Lemmatization ==> Digit removal ==> Feature Extraction (Tf-Idf) ==> Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning(Data Preparation)\n",
    "* __Removal of stop words__\n",
    "Stop words like “and”, “if”, “the”, etc are very common in all English sentences and are not very meaningful in deciding the theme of the article, so these words can be removed from the articles.\n",
    "\n",
    "* __Removal of Punctuation Characters__\n",
    "– Exclude all punctuation marks from the set([‘!’, ‘#’, ‘”‘, ‘%’, ‘$’, “‘”, ‘&’, ‘)’, ‘(‘, ‘+’, ‘*’, ‘-‘, ‘,’, ‘/’, ‘.’, ‘;’, ‘:’, ‘=’, ‘<‘, ‘?’, ‘>’, ‘@’, ‘[‘, ‘]’, ‘\\\\’, ‘_’, ‘^’, ‘`’, ‘{‘, ‘}’, ‘|’, ‘~’]).\n",
    "\n",
    "* __Lemmatization__\n",
    "– It is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. For example, “include”, “includes,” and “included” would all be represented as “include”. The context of the sentence is also preserved in lemmatization as opposed to stemming (another buzz word in text mining which does not consider the meaning of the sentence).\n",
    "\n",
    "* __Removal__ of digits from the text sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in g:\\setupdir\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in g:\\setupdir\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pooya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english')) # | {'two','one'}\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    " \n",
    "# Cleaning the text sentences so that punctuation marks, stop words & digits are removed\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    processed = re.sub(r\"\\d+\",\"\",normalized)\n",
    "    y = processed.split()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Feature Extraction\n",
    "The most popular and widely used word weighing scheme in text mining problems, __term frequency and inverse document frequency (tf-idf)__, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf-idf value increases proportionally to the number of times a word appears in the document (tf), but is often offset by the frequency of the word in the whole corpus (idf), which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "\n",
    "\n",
    "__TF__:\n",
    "    Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
    "\n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "__IDF__:\n",
    "    Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "__Example:__\n",
    "    Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pooya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 sentences of following three classes on which K-means clustering is performed : \n",
      "1. Cricket \n",
      "2. Artificial Intelligence \n",
      "3. Chemistry\n"
     ]
    }
   ],
   "source": [
    "print(\"There are 10 sentences of following three classes on which K-means clustering\"\\\n",
    "         \" is performed : \\n1. Cricket \\n2. Artificial Intelligence \\n3. Chemistry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Sentences.txt\"\n",
    " \n",
    "train_clean_sentences = []\n",
    "fp = open(path,'r')\n",
    "for line in fp:\n",
    "    line = line.strip()\n",
    "    cleaned = clean(line)\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    train_clean_sentences.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cricket bat ball game played two team eleven player cricket field'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(train_clean_sentences)\n",
    " \n",
    "# Creating true labels for 30 training sentences\n",
    "y_train = np.zeros(30) # \n",
    "y_train[10:20] = 1\n",
    "y_train[20:30] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.74084002, 3.74084002, 3.33537492, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.33537492, 2.64222774, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.33537492, 2.82454929, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 2.64222774, 3.74084002,\n",
       "       2.82454929, 2.82454929, 3.74084002, 3.74084002, 3.33537492,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.33537492, 3.74084002, 3.74084002, 2.82454929, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.33537492, 3.74084002,\n",
       "       3.74084002, 2.64222774, 2.82454929, 3.33537492, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.33537492,\n",
       "       3.04769284, 3.74084002, 3.33537492, 3.74084002, 3.74084002,\n",
       "       3.33537492, 3.33537492, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.04769284, 3.74084002, 3.04769284,\n",
       "       3.74084002, 3.33537492, 3.74084002, 3.74084002, 3.33537492,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.04769284, 3.74084002, 3.74084002,\n",
       "       3.33537492, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.33537492, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.33537492, 3.04769284, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.33537492, 3.04769284, 3.04769284,\n",
       "       3.74084002, 3.74084002, 3.33537492, 3.74084002, 3.74084002,\n",
       "       3.33537492, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       2.64222774, 3.33537492, 3.74084002, 3.74084002, 3.33537492,\n",
       "       3.74084002, 3.74084002, 3.33537492, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.33537492, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.33537492, 3.74084002, 3.33537492,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.33537492,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.33537492, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 2.48807706, 3.74084002, 3.74084002,\n",
       "       3.33537492, 3.74084002, 3.74084002, 3.74084002, 3.04769284,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.33537492,\n",
       "       2.64222774, 3.74084002, 3.04769284, 3.74084002, 3.04769284,\n",
       "       3.33537492, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.33537492, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.33537492, 3.33537492,\n",
       "       3.74084002, 3.33537492, 3.04769284, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 2.35454566, 3.33537492,\n",
       "       3.33537492, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002, 3.74084002, 3.74084002,\n",
       "       3.74084002, 3.74084002, 3.74084002])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'access',\n",
       " 'action',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancement',\n",
       " 'agent',\n",
       " 'ai',\n",
       " 'allows',\n",
       " 'analytical',\n",
       " 'animal',\n",
       " 'apiece',\n",
       " 'artificial',\n",
       " 'atom',\n",
       " 'atomic',\n",
       " 'attempting',\n",
       " 'autonomous',\n",
       " 'bail',\n",
       " 'ball',\n",
       " 'basic',\n",
       " 'bat',\n",
       " 'batsman',\n",
       " 'batting',\n",
       " 'bear',\n",
       " 'begin',\n",
       " 'behaviour',\n",
       " 'biology',\n",
       " 'bond',\n",
       " 'boundary',\n",
       " 'bowled',\n",
       " 'bowler',\n",
       " 'branch',\n",
       " 'bridge',\n",
       " 'called',\n",
       " 'captain',\n",
       " 'car',\n",
       " 'central',\n",
       " 'challenging',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'charged',\n",
       " 'chemical',\n",
       " 'chemistry',\n",
       " 'cloud',\n",
       " 'coin',\n",
       " 'commercial',\n",
       " 'common',\n",
       " 'composed',\n",
       " 'composition',\n",
       " 'compound',\n",
       " 'computer',\n",
       " 'configuration',\n",
       " 'consists',\n",
       " 'contributed',\n",
       " 'core',\n",
       " 'create',\n",
       " 'cricket',\n",
       " 'cross',\n",
       " 'data',\n",
       " 'deep',\n",
       " 'defines',\n",
       " 'delivers',\n",
       " 'dense',\n",
       " 'determine',\n",
       " 'determined',\n",
       " 'developed',\n",
       " 'device',\n",
       " 'different',\n",
       " 'directly',\n",
       " 'dislodges',\n",
       " 'dismissal',\n",
       " 'drone',\n",
       " 'electron',\n",
       " 'electronegativity',\n",
       " 'element',\n",
       " 'enabled',\n",
       " 'end',\n",
       " 'enter',\n",
       " 'entity',\n",
       " 'environment',\n",
       " 'envisaged',\n",
       " 'essential',\n",
       " 'exhibited',\n",
       " 'expert',\n",
       " 'extract',\n",
       " 'faster',\n",
       " 'fewer',\n",
       " 'field',\n",
       " 'fielder',\n",
       " 'fielding',\n",
       " 'form',\n",
       " 'function',\n",
       " 'game',\n",
       " 'gas',\n",
       " 'geology',\n",
       " 'goal',\n",
       " 'growth',\n",
       " 'hard',\n",
       " 'helping',\n",
       " 'hit',\n",
       " 'hitting',\n",
       " 'human',\n",
       " 'includes',\n",
       " 'including',\n",
       " 'individual',\n",
       " 'industry',\n",
       " 'inertness',\n",
       " 'inning',\n",
       " 'intelligence',\n",
       " 'intelligent',\n",
       " 'ionization',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'language',\n",
       " 'large',\n",
       " 'learning',\n",
       " 'likewise',\n",
       " 'little',\n",
       " 'loosely',\n",
       " 'lost',\n",
       " 'machine',\n",
       " 'main',\n",
       " 'make',\n",
       " 'manner',\n",
       " 'match',\n",
       " 'mathematical',\n",
       " 'matter',\n",
       " 'maximize',\n",
       " 'measurable',\n",
       " 'meet',\n",
       " 'member',\n",
       " 'method',\n",
       " 'natural',\n",
       " 'negatively',\n",
       " 'neutron',\n",
       " 'noble',\n",
       " 'nucleon',\n",
       " 'nucleus',\n",
       " 'objective',\n",
       " 'opponent',\n",
       " 'orbit',\n",
       " 'overall',\n",
       " 'oxidation',\n",
       " 'pattern',\n",
       " 'perceives',\n",
       " 'perception',\n",
       " 'periodic',\n",
       " 'phase',\n",
       " 'physic',\n",
       " 'physical',\n",
       " 'pitch',\n",
       " 'play',\n",
       " 'played',\n",
       " 'player',\n",
       " 'positively',\n",
       " 'possible',\n",
       " 'potential',\n",
       " 'preferred',\n",
       " 'prevent',\n",
       " 'problem',\n",
       " 'processing',\n",
       " 'program',\n",
       " 'property',\n",
       " 'proton',\n",
       " 'pure',\n",
       " 'reaction',\n",
       " 'read',\n",
       " 'recent',\n",
       " 'recurring',\n",
       " 'research',\n",
       " 'researcher',\n",
       " 'result',\n",
       " 'retain',\n",
       " 'revived',\n",
       " 'role',\n",
       " 'run',\n",
       " 'said',\n",
       " 'science',\n",
       " 'scientific',\n",
       " 'score',\n",
       " 'scored',\n",
       " 'selfdriving',\n",
       " 'sense',\n",
       " 'similarity',\n",
       " 'simulated',\n",
       " 'simultaneously',\n",
       " 'skill',\n",
       " 'smallest',\n",
       " 'solve',\n",
       " 'sophisticated',\n",
       " 'space',\n",
       " 'specific',\n",
       " 'specifically',\n",
       " 'state',\n",
       " 'statistical',\n",
       " 'strike',\n",
       " 'striker',\n",
       " 'structure',\n",
       " 'study',\n",
       " 'stump',\n",
       " 'subproblems',\n",
       " 'substance',\n",
       " 'success',\n",
       " 'suitable',\n",
       " 'surrounded',\n",
       " 'swap',\n",
       " 'swapping',\n",
       " 'systems',\n",
       " 'team',\n",
       " 'technique',\n",
       " 'technology',\n",
       " 'thing',\n",
       " 'tool',\n",
       " 'topic',\n",
       " 'toss',\n",
       " 'transformation',\n",
       " 'truly',\n",
       " 'uncharged',\n",
       " 'understand',\n",
       " 'unit',\n",
       " 'using',\n",
       " 'verifiable',\n",
       " 'wanted']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Clustering (K-Means) models.\n",
    "As we have discussed earlier also, Text classification is a supervised learning task, whereas text clustering is an unsupervised task. We are investigating one machine learning algorithms here: K-Means clustering.\n",
    "The goal of clustering is to determine the intrinsic grouping in a set of unlabeled data (feature vectors). In K-Means clustering, ‘K’ cluster centers are discovered which is centroid of data points belonging to that cluster. A test data (feature-vector) is assigned to that cluster whose centroid is at minimum Euclidean distance from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=200,\n",
       "       n_clusters=3, n_init=100, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clustering the training 30 sentences with K-means technique\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=200, n_init=100)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Unseen Texts.\n",
    "Once the model has been trained,  we demonstrate the concept of classification and clustering with above conventional methods. We tested it on the following few unseen text sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting it on test data : Testing Phase\n",
    "test_sentences = [\"Chemical compunds are used for preparing bombs based on some reactions\",\\\n",
    "\"Cricket is a boring game where the batsman only enjoys the game\",\\\n",
    "\"Machine learning is an area of Artificial intelligence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------PREDICTIONS BY K-Means--------------------------------------\n",
      "\n",
      "Index of Cricket cluster :  1\n",
      "Index of Artificial Intelligence cluster :  2\n",
      "Index of Chemistry cluster :  0\n",
      "\n",
      " Chemical compunds are used for preparing bombs based on some reactions : 0 \n",
      " Cricket is a boring game where the batsman only enjoys the game : 1 \n",
      " Machine learning is a area of Artificial intelligence : 2\n"
     ]
    }
   ],
   "source": [
    "test_clean_sentence = []\n",
    "for test in test_sentences:\n",
    "    cleaned_test = clean(test)\n",
    "    cleaned = ' '.join(cleaned_test)\n",
    "    cleaned = re.sub(r\"\\d+\",\"\",cleaned)\n",
    "    test_clean_sentence.append(cleaned)\n",
    "\n",
    "Test = vectorizer.transform(test_clean_sentence)\n",
    "true_test_labels = ['Cricket','AI','Chemistry']\n",
    "predicted_labels_kmeans = kmeans.predict(Test)\n",
    " \n",
    "print( \"\\n-------------------------------PREDICTIONS BY K-Means--------------------------------------\")\n",
    "print( \"\\nIndex of Cricket cluster : \",Counter(kmeans.labels_[0:10]).most_common(1)[0][0])\n",
    "print( \"Index of Artificial Intelligence cluster : \",Counter(kmeans.labels_[10:20]).most_common(1)[0][0])\n",
    "print( \"Index of Chemistry cluster : \",Counter(kmeans.labels_[20:30]).most_common(1)[0][0])\n",
    " \n",
    "print( \"\\n\",test_sentences[0],\":\",predicted_labels_kmeans[0],\\\n",
    "\"\\n\",test_sentences[1],\":\",predicted_labels_kmeans[1],\\\n",
    "\"\\n\",test_sentences[2],\":\",predicted_labels_kmeans[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
